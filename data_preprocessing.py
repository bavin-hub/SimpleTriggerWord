# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LtcYEBSl_casY0eV2jlauAFaeBxEOtdk
"""

from google.colab import drive, files
drive.mount("/content/drive")

!pip install librosa

# Commented out IPython magic to ensure Python compatibility.
import librosa, librosa.display

from IPython.display import Audio, display

import numpy as np
import os
import matplotlib.pyplot as plt
import random
# %matplotlib inline

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation, LSTM
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.utils import to_categorical

activate = "/content/drive/MyDrive/explorer_main/final_trigger_word/audio_dataset/activates/activate0.wav"
display(Audio(activate))

noise = "/content/drive/MyDrive/explorer_main/final_trigger_word/audio_dataset/noises/background0.wav"
display(Audio(noise))

data, sr = librosa.load(activate)
print(data.shape)
print(sr)

plt.plot("waveform")
librosa.display.waveplot(data, sr=sr)
plt.show()

mfcc = librosa.feature.mfcc(data, sr=sr, n_mfcc=40)
plt.plot("mfcc")
librosa.display.specshow(mfcc, sr=sr, x_axis="time")
plt.show()

class Preprocess():
  def __init__(self, audio_path_list, n_mfcc):
    self.audio_path_list = audio_path_list
    self.n_mfcc = n_mfcc
  
  def single_audio_feature(self, single_audio_path):
    self.data, self.sr = librosa.load(single_audio_path)
    self.mfcc = librosa.feature.mfcc(self.data, sr=self.sr, n_mfcc=self.n_mfcc)
    self.reduced_mfcc = np.mean(self.mfcc.T, axis=0)
    self.reduced_mfcc = self.reduced_mfcc
    return self.reduced_mfcc
  
  def create_dataset(self):
    self.dataset = []
    for i in range(len(self.audio_path_list)):
      self.dir_names = self.audio_path_list[i].split("/") 
      if self.dir_names[-1]=="activates":
        for audio in os.listdir(self.audio_path_list[i]):
          self.x = self.single_audio_feature(f"{self.audio_path_list[i]}/{audio}")
          self.y = 1
          self.dataset.append([self.x, self.y])
      else:
        for audio in os.listdir(self.audio_path_list[i]):
          self.x = self.single_audio_feature(f"{self.audio_path_list[i]}/{audio}")
          self.y = 0
          self.dataset.append([self.x, self.y])
    return self.dataset

audio = ["/content/drive/MyDrive/explorer_main/final_trigger_word/audio_dataset/activates",
         "/content/drive/MyDrive/explorer_main/final_trigger_word/audio_dataset/noises"]

preprocessor = Preprocess(audio_path_list=audio, n_mfcc=40)
reduced_mfcc = preprocessor.single_audio_feature(activate)

print(reduced_mfcc)
print(reduced_mfcc.shape)
print(type(reduced_mfcc))

dataset = preprocessor.create_dataset()

print("dataset contains {} training examples".format(len(dataset)))

random.seed(43)
random.shuffle(dataset)

train_dataset = dataset[:1000]
valid_dataset = dataset[1000:]

def process_dataset(data):
  x = []
  y = []
  for features, label in data:
    x.append(features)
    y.append(label)
  x = np.array(x)
  y = np.array(y)
  return x, y

x_train, y_train = process_dataset(train_dataset)

x_valid, y_valid = process_dataset(valid_dataset)

save_path = "/content/drive/MyDrive/explorer_main/final_trigger_word/preprocessed_data"

np.save(f"{save_path}/x_train.npy", x_train)
np.save(f"{save_path}/y_train.npy", y_train)

np.save(f"{save_path}/x_valid.npy", x_valid)
np.save(f"{save_path}/y_valid.npy", y_valid)







